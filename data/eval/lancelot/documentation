Collation et évalutation :


- scriptBase.ipynb s'exécute dans jupyter, directement sur le texte
- scriptQuiFonctionne.py travaille sur les lemmes, via des csv
- j'ai repris la fonction de production d'HTML d'Elena, qui se trouve dans fonction_Elena.py
- scriptQuiFonctionne.py  : on le modifie selon qu'on veuille travailler sur les lemmes corrigés ou non corrigés

- dans data : 
	- les deux textes en version .txt
	- les deux csv avec les lemmes corrigés
	- les deux csv avec les lemmes non-corrigés (versions -pie)

- dans le dossier results : les deux doc HTML produits par scriptQuiFonctionne.py (version lemmes corrigés/non corrigés) -> on a pris ces tables pour l'évaluation

- scriptBase.ipynb : directement sur les versions .txt, on récupère la table produite dans le notebook pour l'évaluation

-----------------------------------------
- l'évaluation se trouve dans les fichiers .ods.

- les versions corrigées et non corrigées ont les mêmes erreurs (voir si systématique en traitant d'autres segments ?) :
	- 41 erreurs sur 510, soit un taux de :	8,03921568627451 (nombre de cases fautives sur nombre totale de cases)
	- 11 erreurs pour A, 30 pour B, soit un taux d’erreurs de B à :	73,1707317073171
--> le score est donc bon

- la version sur forme a un taux d'erreurs beaucoup plus élevé : 
	- fautes totales : 91 sur 506, soit un taux d’erreurs de :	17,9841897233202
	 - 61 pour B, 30 pour A, soit une portion d’erreurs de :	67,032967032967


- les taux plus élevés de B par rapport à A s'expliquent puisque B est collationné par rapport à A


- certaines erreurs sont similaires : par exemple, les trois versions collationnent ainçois avec je alors qu'on aimerait ançois/- -/je m/m an/en iroie/iray ge/- (pour marquer la transposition)

- beaucoup d'erreurs sont évitées grâce aux lemmes : la version sur formes donne : fuiez/pour -/dieu -/fuyez alors que les versions à lemmes donnent la bonne collation

- les versions à lemmes insèrent plus d'omission pour mieux aligner

--> la lemmatisation est efficace !


-----------------------
- peu d'erreurs dans l'annotation linguistique : 6 erreurs sur 338 tokens + 3 d'entre elles sont des mots qui peuvent en effet avoir ce type d'étiquette
	- vivant : vivre1_NOMcom contre vivant_NOMcom
	- vit : vif_VERcjg contre vif_ADJqua
	- c : que1_CONsub contre que4_CONsub
	- o : o4_CONcoo contre o3_CONcoo
	- rien : rien_PROind contre rien_NOMcom
	- qu : que4_PROrel contre que4_CONsub


